{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np\n",
    "import thermocore.io.casm as cio\n",
    "from djlib.plotting.hull_plotting import general_binary_convex_hull_plotter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "import thermocore.geometry.hull as thull\n",
    "import djlib.clex.clex as cl\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the query data from the json file\n",
    "with open('ZrN_FCC_1.2.0_TITAN_calculated.json') as f:\n",
    "    query = json.load(f)\n",
    "data = cio.regroup_query_by_config_property(query)\n",
    "corr = np.array(data['corr'])\n",
    "corr_unaltered = np.array(data['corr'])\n",
    "comp = np.array(data['comp'])\n",
    "formation_energy = np.array(data['formation_energy'])\n",
    "name = np.array(data['name'])\n",
    "print(corr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Often, we will work with a large set of basis functions, and remove basis functions that are not needed. \n",
    "#However, CASM will expect an ECI vector that is the same length as the original set of basis functions.\n",
    "#Maintaining an upscaling vector of booleans allows us to record the basis functions that are removed, and\n",
    "#reconstruct the original ECI vector when needed.\n",
    "upscaling_boolean_vector = np.array([True]*corr.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dealing with a CASM bug: large clusters with multiplicity ==1 produce problems in monte carlo. Remove them from the fit.\n",
    "\n",
    "\n",
    "#Load the basis.json file to find the multiplicity of each correlation\n",
    "#Normally, this is located in project_root/basis_sets/bset.default/basis.json\n",
    "basis_path = './basis.json'\n",
    "with open(basis_path) as f:\n",
    "    basis = json.load(f)\n",
    "multiplicities = np.array(cio.pull_multiplicity_from_json(basis))\n",
    "print(np.where(multiplicities == 1))\n",
    "\n",
    "#Current bug in casm: large clusters with multiplicity ==1 produce problems in monte carlo. \n",
    "#Find clusters with multiplicity == 1; if the index is greater than 10, remove, store that index\n",
    "large_multiplicity_1_clusters = np.where(multiplicities == 1)[0]\n",
    "large_multiplicity_1_clusters = large_multiplicity_1_clusters[large_multiplicity_1_clusters > 10]\n",
    "print(large_multiplicity_1_clusters)\n",
    "\n",
    "#Remove clusters with multiplicity == 1 and index > 10 from corr\n",
    "upscaling_boolean_vector[large_multiplicity_1_clusters] = False\n",
    "print(upscaling_boolean_vector.shape)\n",
    "print(corr.shape)\n",
    "corr = corr[:,upscaling_boolean_vector]\n",
    "print(corr.shape)\n",
    "\n",
    "print('\\nLarge clusters with multiplicity == 1 have been removed from corr.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the fully enumerated data:\n",
    "with open('ZrN_FCC_1.2.0_TITAN_67k_uncalculated_3-15-2023.json') as f:\n",
    "    query = json.load(f)\n",
    "data = cio.regroup_query_by_config_property(query)\n",
    "corr_full = np.array(data['corr'])\n",
    "corr_full_unaltered = np.array(data['corr'])\n",
    "comp_full = np.array(data['comp'])\n",
    "name_full = np.array(data['name'])\n",
    "\n",
    "print(corr_full.shape)\n",
    "\n",
    "#Remove large clusters with multiplicity == 1 from corr_full\n",
    "corr_full = corr_full[:,upscaling_boolean_vector]\n",
    "print(corr_full.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To find models that replicate DFT ground states, we need to identify the DFT ground states. \n",
    "#Additionally, if we want to prioritize models that are close to the convex hull, we need to quantify the distance of each model from the convex hull.\n",
    "#Hull distances are used for weighted least squares, but they are also used for calculating hull distance correlations. \n",
    "\n",
    "\n",
    "#Calculate hull distances for each point\n",
    "hull = thull.full_hull(compositions=comp, energies=formation_energy)\n",
    "hull_distances = thull.lower_hull_distances(convex_hull=hull, compositions=comp, energies=formation_energy)\n",
    "\n",
    "#Calculate true convex hull vertices\n",
    "true_lower_hull_vertices, _ = thull.lower_hull(hull)\n",
    "print(true_lower_hull_vertices)\n",
    "print(comp[true_lower_hull_vertices])\n",
    "print(name[true_lower_hull_vertices])\n",
    "\n",
    "#Calculate hull distance correlations\n",
    "hull_corr = thull.hull_distance_correlations(corr=corr, compositions=comp, formation_energy=formation_energy, hull=hull)\n",
    "feature_matrix = np.concatenate((corr, hull_corr), axis=0)\n",
    "target_values = np.concatenate((formation_energy, hull_distances), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The fully enumerated dataset contains all calculated and uncalculated structures. \n",
    "#However, the same structures may have different indices in the calculated and uncalculated datasets.\n",
    "#We know the hull vertex indices in the calculated dataset; find their corresponding index in the uncalculated dataset.\n",
    "\n",
    "#The correlation vector for a structure is the same in both the calculated and uncalculated datasets.\n",
    "#Therefore, we can use the correlation vector to find the index of the hull vertex in the uncalculated dataset.\n",
    "\n",
    "#Find the correlation vector for each hull vertex in the calculated dataset\n",
    "hull_vertices_corr = corr[true_lower_hull_vertices,:]\n",
    "print(hull_vertices_corr.shape)\n",
    "\n",
    "#Find the index of each hull vertex in the uncalculated dataset\n",
    "hull_vertex_indices_full_enumeration = []\n",
    "for i in range(hull_vertices_corr.shape[0]):\n",
    "    hull_vertex_indices_full_enumeration.append(np.where((corr_full == hull_vertices_corr[i]).all(axis=1))[0][0])\n",
    "hull_vertex_indices_full_enumeration = np.array(hull_vertex_indices_full_enumeration)\n",
    "print(name_full[hull_vertex_indices_full_enumeration])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model that we will use to fit the data.\n",
    "reg = LassoLarsCV(fit_intercept=False, n_jobs=-1, max_iter=50000)\n",
    "\n",
    "#Weighted leas squares requires a weighting scheme. We will weight by the Boltzmann distribution of the hull distances.\n",
    "#We must still specify how \"aggressively\" we want to weight the data. This is done by specifying a \"B\" value.\n",
    "#Larger B values will weight the data more aggressively, placing more emphasis on data that is close to the convex hull.\n",
    "#Specify an array of boltzmann \"B\" weights to use: linearly spaced from 1 to 300, 100 points\n",
    "B = np.linspace(1, 300, 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each B, calculate the weighted feature and target arrays. \n",
    "#Then fit a cluster expansion model with LassoLarsCV\n",
    "models = []\n",
    "for b in B:\n",
    "    #calculate the weighted feature and target vectors\n",
    "    weight = np.identity(target_values.shape[0])\n",
    "    for config_index in range(formation_energy.shape[0]):\n",
    "        weight[config_index, config_index] = cl.boltzmann(\n",
    "            hulldist=hull_distances[config_index], coef=1, beta=b, temperature=1.05\n",
    "        )\n",
    "        weight[config_index+formation_energy.shape[0], config_index+formation_energy.shape[0]] = cl.boltzmann(\n",
    "            hulldist=hull_distances[config_index], coef=1, beta=b, temperature=1.05\n",
    "        )\n",
    "    x_prime, y_prime = cl.general_weighted_feature_and_target_arrays(feature_matrix=feature_matrix, target_array=target_values, weight_matrix=weight)\n",
    "    \n",
    "    #fit the model\n",
    "    reg.fit(x_prime, y_prime)\n",
    "    models.append(reg.coef_)\n",
    "\n",
    "models = np.array(models)\n",
    "print(models.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump the models to a pickle file, just so that we don't have to do this every time we open the notebook.\n",
    "print(models.shape)\n",
    "with open('weighted_LassoLarsCV_models.json', 'w') as f:\n",
    "    json.dump({'models':models.tolist()}, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load models from the pickle file\n",
    "with open('weighted_LassoLarsCV_models.json', 'r') as f:\n",
    "    models = np.array(json.load(f)['models'])\n",
    "print(models.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the predicted energies for each model \n",
    "predicted_energies = corr @ models.T\n",
    "predicted_energies = predicted_energies.T\n",
    "\n",
    "predicted_energies_full_enumeration = corr_full @ models.T\n",
    "predicted_energies_full_enumeration = predicted_energies_full_enumeration.T\n",
    "\n",
    "print(predicted_energies.shape)\n",
    "print(predicted_energies_full_enumeration.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each row in the predicted energies array, compute: \n",
    "#rmse, ground_state_accuracy_metric, ground_state_fraction_correct, gsa_fraction_correct_DFT_mu_window_binary, gsa_intersection_over_union, complexity\n",
    "\n",
    "max_rmse_tolerance = 0.030 #eV/prim\n",
    "\n",
    "rmse = []\n",
    "gsad = []\n",
    "fraction_correct_weighted_full_enumeration = []\n",
    "complexity = []\n",
    "\n",
    "for index, row in enumerate(predicted_energies):\n",
    "    #calculate predicted ground state indices\n",
    "    predicted_hull = thull.full_hull(compositions=comp, energies=row)\n",
    "    predicted_lower_hull_vertices, _ = thull.lower_hull(predicted_hull)\n",
    "\n",
    "    #calculate the rmse\n",
    "    rmse.append(np.sqrt(mean_squared_error(formation_energy, row)))\n",
    "\n",
    "    #calculate the ground state accuracy metric with a DFT mu window\n",
    "    gsad.append(\n",
    "        cl.gsa_fraction_correct_DFT_mu_window_binary(predicted_comp=comp,\n",
    "                                                     predicted_energies=row,\n",
    "                                                     predicted_corr=corr, \n",
    "                                                     true_comp=comp, \n",
    "                                                     true_energies=formation_energy,\n",
    "                                                     true_corr=corr)\n",
    "        )\n",
    "    \n",
    "    #calculate the model complexity: number of nonzero ECI\n",
    "    complexity.append(np.count_nonzero(models[index,:])/len(models[index,:]))\n",
    "\n",
    "rmse = np.array(rmse)\n",
    "gsad = np.array(gsad)\n",
    "complexity = np.array(complexity)\n",
    "\n",
    "\n",
    "for index, row in enumerate(predicted_energies_full_enumeration):\n",
    "    predicted_hull_full_enumeration = thull.full_hull(compositions=comp_full, energies=row)\n",
    "    predicted_lower_hull_vertices_full_enumeration, _ = thull.lower_hull(predicted_hull_full_enumeration)\n",
    "\n",
    "    fraction_correct_weighted_full_enumeration.append(\n",
    "        cl.gsa_fraction_correct_DFT_mu_window_binary(predicted_comp=comp_full, \n",
    "                                                     predicted_energies=row,\n",
    "                                                     predicted_corr=corr_full,\n",
    "                                                     true_comp=comp,\n",
    "                                                     true_energies=formation_energy,\n",
    "                                                     true_corr=corr)\n",
    "        )\n",
    "fraction_correct_weighted_full_enumeration = np.array(fraction_correct_weighted_full_enumeration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot all metrics vs B\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 12))\n",
    "print('Max error: ', max(rmse),' eV')\n",
    "\n",
    "\n",
    "ax.plot(B, 1-np.array(rmse)/max_rmse_tolerance, label='Normalized accuracy')\n",
    "ax.plot(B, gsad, label='gs fraction correct DFT mu window CALCULATED ONLY')\n",
    "ax.plot(B, 1-np.array(complexity), label='Fraction of ECI pruned')\n",
    "ax.plot(B, fraction_correct_weighted_full_enumeration, label='gs fraction correct DFT mu window FULL ENUMERATION')\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xlabel('B', fontsize=21)\n",
    "ax.set_ylabel('metric value', fontsize=21)\n",
    "ax.legend(fontsize=21)\n",
    "plt.ylim([0,1])\n",
    "plt.xticks(fontsize=21)\n",
    "plt.yticks(fontsize=21)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are primarily interested in four metrics:\n",
    "#1. Fraction correct, weighted by stable chemical potential window. This is labelled 'gsad' in the dataframe.\n",
    "#2. Intersection over union, weighted by stable chemical potential window. This is labelled 'gsa' in the dataframe.\n",
    "#3. Normalized accuracy.\n",
    "#4. Fraction of ECI pruned.\n",
    "#All of these metrics are normalized [0,1], where 1 is the best possible value.\n",
    "#Create a composite metric by multiplying some, or all metrics together, and then plot the result vs B.\n",
    "\n",
    "product = fraction_correct_weighted_full_enumeration * (1-np.array(rmse)/max_rmse_tolerance) * (1-np.array(complexity))\n",
    "\n",
    "print(np.argmax(product), max(product))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 12))\n",
    "ax.plot(B, product, label='product', color='k')\n",
    "ax.set_xlabel('B', fontsize=21)\n",
    "ax.set_ylabel('Composite Metric', fontsize=21)\n",
    "plt.xticks(fontsize=21)\n",
    "plt.yticks(fontsize=21)\n",
    "ax.legend(fontsize=21)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the  maximum value of the combined metric and the corresponding B\n",
    "\n",
    "max_index = np.argmax(product)\n",
    "max_value = max(product)\n",
    "max_B = B[max_index]\n",
    "\n",
    "\n",
    "print(max_value, max_B, max_index)\n",
    "\n",
    "\n",
    "#Rank the modls by the combined metric: print the indices of the top 10 models, just in case you want to look at them.\n",
    "for i in np.argsort(product)[::-1][:10]:\n",
    "    print(i, product[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the model with the maximum value of the combined metric\n",
    "max_model = models[max_index]\n",
    "\n",
    "#Add a manual override for the maximum index, in case you want to look at a specific model\n",
    "manual_max_index = False #0\n",
    "if manual_max_index is not False:\n",
    "    max_model = models[manual_max_index]\n",
    "    max_index = manual_max_index\n",
    "\n",
    "print(max_model.shape)\n",
    "\n",
    "#Write the max model to a file\n",
    "with open('max_model.json', 'w') as f:\n",
    "    json.dump({'max_model':max_model.tolist(), 'upscaling_vector':upscaling_boolean_vector.tolist()}, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the optimal model\n",
    "print('There are {} non-zero coefficients in the model'.format(np.count_nonzero(max_model)))\n",
    "from djlib.plotting.hull_plotting import general_binary_convex_hull_plotter\n",
    "\n",
    "print(np.count_nonzero(max_model))\n",
    "\n",
    "\n",
    "\n",
    "print(cl.gsa_fraction_correct_DFT_mu_window_binary(predicted_comp=comp_full.reshape(-1,1),\n",
    "                                                   predicted_energies=np.ravel(predicted_energies_full_enumeration[max_index]), \n",
    "                                                   predicted_corr=corr_full,\n",
    "                                                   true_comp=comp.reshape(-1,1),\n",
    "                                                   true_energies= np.ravel(formation_energy),\n",
    "                                                   true_corr=corr)\n",
    "                                                   )\n",
    "\n",
    "fig = general_binary_convex_hull_plotter(comp, formation_energy, predicted_energies=predicted_energies[max_index])\n",
    "fig.set_size_inches(15,12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the ECI of the optimal model\n",
    "#The plot is divided by vertical dashed red lines, denoting the empty, point, pair, triplet, etc. clusters. \n",
    "\n",
    "\n",
    "#load the basis.json file to access site multiplicity in each cluster\n",
    "with open('basis.json', 'r') as f:\n",
    "    basis = json.load(f)\n",
    "print(basis.keys())\n",
    "print(basis['orbits'][0]['prototype']['sites'])\n",
    "\n",
    "cluster_sizes = np.array([len(basis['orbits'][i]['prototype']['sites']) for i in range(len(basis['orbits']))])\n",
    "print(cluster_sizes)\n",
    "\n",
    "#find the indices where the cluster size increments by 1\n",
    "cluster_size_indices = np.where(np.diff(cluster_sizes)==1)[0]\n",
    "print(cluster_size_indices)\n",
    "\n",
    "\n",
    "plt.scatter(list(range(max_model.shape[0])), max_model, color='k')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(20, 12)\n",
    "plt.xticks(fontsize=21)\n",
    "plt.yticks(fontsize=21)\n",
    "plt.xlabel('ECI index', fontsize=21)\n",
    "plt.ylabel('ECI value', fontsize=21)\n",
    "#Plot a vertical dashed line at each cluster size increment\n",
    "for i in cluster_size_indices:\n",
    "    plt.axvline(x=i+.5, color='r', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once a prior mean is decided, a prior standard deviation must be chosen.  \n",
    "import djlib.djlib as dj\n",
    "eci_stddev_values = np.linspace(0.00001, 0.05, 1000)         #stddev in eV\n",
    "prior_mean = max_model\n",
    "likelihood_stddev = 0.005                               #stddev in eV\n",
    "\n",
    "#Collect all indices of nonzero coefficients and store them in an array called nonzero_indices\n",
    "#The prior mean must first be upscaled back to the original feature size (because of the casm bug)\n",
    "full_dimension_prior_mean = cio.upscale_eci_vector(prior_mean, upscaling_boolean_vector)\n",
    "print(full_dimension_prior_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the coefficient indices that have been pruned. Store them to allow later upscaling\n",
    "zero_indices = np.where(full_dimension_prior_mean == 0)[0]\n",
    "zeroed_upscaling_boolean_vector = np.copy(upscaling_boolean_vector)\n",
    "zeroed_upscaling_boolean_vector[zero_indices] = False\n",
    "\n",
    "\n",
    "#downsample the correlation matrix to only include nonzero coefficients\n",
    "corr_nonzero = corr_unaltered[:, np.where(zeroed_upscaling_boolean_vector)[0]]\n",
    "corr_full_nonzero = corr_full_unaltered[:, np.where(zeroed_upscaling_boolean_vector)[0]]\n",
    "print(corr_nonzero.shape)\n",
    "print(corr_full_nonzero.shape)\n",
    "\n",
    "#downsample the prior mean to only include nonzero coefficients\n",
    "prior_mean_nonzero = full_dimension_prior_mean[np.where(zeroed_upscaling_boolean_vector)[0]]\n",
    "print(prior_mean_nonzero.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are multiple ways to choose the prior standard deviation. \n",
    "#One way is to start the prior at the prior mean, and iteratively broaden the standard deviaiton, \n",
    "#monitoring the ground state accuracy as the prior distribution is broadened. \n",
    "\n",
    "\n",
    "\n",
    "#It is sometimes possible for this sampling to find a model with a better ground state accuracy. \n",
    "#If the \"prior_data\" pkl file exists, load it and use the max_gsa_model as the new prior_mean_nonzero\n",
    "\n",
    "if os.path.exists('prior_data.pkl'):\n",
    "    with open('prior_data.pkl', 'rb') as f:\n",
    "        prior_data = pickle.load(f)\n",
    "    prior_mean_nonzero = prior_data['max_gsa_model']\n",
    "    print(prior_mean_nonzero.shape)\n",
    "    print(prior_mean_nonzero)\n",
    "    print('Loaded prior data from prior_data.pkl')\n",
    "\n",
    "#For each value of the prior variance, define a prior covariance matrix as the identity matrix multiplied by the variance.\n",
    "prior_data = {\"rmse_means\":[],\"rmse_stddevs\":[], \"fraction_correct_weighted_means\":[],\"fraction_correct_weighted_stddevs\":[] }\n",
    "\n",
    "\n",
    "#Store the model with the maximum ground state accuracy; every sample, check if the current model has a higher ground state accuracy than the previous maximum. \n",
    "# If so, replace the previous model with the current model.\n",
    "max_gsa = 0\n",
    "max_gsa_model = None\n",
    "\n",
    "for index, eci_stddev in enumerate(eci_stddev_values):\n",
    "    prior_covariance = eci_stddev ** 2 * np.identity(prior_mean_nonzero.shape[0])\n",
    "    \n",
    "    #Sample from the prior distribution using the prior mean and covariance matrix; take 1000 samples\n",
    "    prior_samples = np.random.multivariate_normal(prior_mean_nonzero, prior_covariance, 10000)\n",
    "\n",
    "    #For each sample, calculate the rmse and ground state accuracy of the model\n",
    "    predicted_energies = corr_nonzero @ prior_samples.T\n",
    "    predicted_energies_full = corr_full_nonzero @ prior_samples.T\n",
    "    \n",
    "    #Calculate the rmse and ground state accuracy of each model\n",
    "    rmse = [mean_squared_error(formation_energy, predicted_energy, squared=False) for predicted_energy in predicted_energies.T]\n",
    "    fraction_correct = [cl.gsa_fraction_correct_DFT_mu_window_binary(predicted_comp=comp_full,\n",
    "                                                                    predicted_energies=predicted_energy_full_vec, \n",
    "                                                                    predicted_corr=corr_full_nonzero,\n",
    "                                                                    true_comp=comp,\n",
    "                                                                    true_energies=formation_energy,\n",
    "                                                                    true_corr=corr_nonzero\n",
    "                                                                    ) for predicted_energy_full_vec in predicted_energies_full.T\n",
    "                                                                    ]\n",
    "\n",
    "    #Check the ground state accuracy of the current model. If it is higher than the previous maximum, replace the previous maximum with the current model.\n",
    "    if max(fraction_correct) > max_gsa:\n",
    "        max_gsa = max(fraction_correct)\n",
    "        max_gsa_model = prior_samples[np.argmax(fraction_correct)]\n",
    "        max_index = index\n",
    "        prior_data[\"max_gsa\"] = max_gsa\n",
    "        prior_data[\"max_gsa_model\"] = max_gsa_model\n",
    "        \n",
    "    #Calculate the mean and standard deviation of the rmse and ground state accuracy\n",
    "    rmse_mean = np.mean(rmse)\n",
    "    rmse_stddev = np.std(rmse)\n",
    "    fraction_correct_weighted_mean = np.mean(fraction_correct)\n",
    "    fraction_correct_weighted_stddev = np.std(fraction_correct)\n",
    "\n",
    "    #Store the mean and standard deviation of the rmse and ground state accuracy\n",
    "    prior_data[\"rmse_means\"].append(rmse_mean)\n",
    "    prior_data[\"rmse_stddevs\"].append(rmse_stddev)\n",
    "    prior_data[\"fraction_correct_weighted_means\"].append(fraction_correct_weighted_mean)\n",
    "    prior_data[\"fraction_correct_weighted_stddevs\"].append(fraction_correct_weighted_stddev)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(prior_data[\"fraction_correct_weighted_means\"]))\n",
    "print(len(prior_data[\"fraction_correct_weighted_stddevs\"]))\n",
    "print(len(prior_data[\"rmse_means\"]))\n",
    "print(eci_stddev_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The previous cell takes ~118 minutes to run. Save all results to a pickle file, so that we don't have to run it again.\n",
    "\n",
    "prior_data[\"eci_stddev_values\"] = eci_stddev_values\n",
    "with open('prior_data.pkl', 'wb') as f:\n",
    "    pickle.dump(prior_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from the pickle file\n",
    "with open('prior_data.pkl', 'rb') as f:\n",
    "    prior_data = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Max GSA: ', prior_data['max_gsa'])\n",
    "max_gsa_model=prior_data['max_gsa_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate predited formation energies for the max_gsa_model, and display the convex hull \n",
    "max_gsa_model_predicted_energies = corr_nonzero @ max_gsa_model\n",
    "max_gsa_model_predicted_energies_full = corr_full_nonzero @ max_gsa_model\n",
    "\n",
    "fig = general_binary_convex_hull_plotter(composition=comp, true_energies=formation_energy, predicted_energies=max_gsa_model_predicted_energies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the ground state accuracy and RMSE statistics as a function of the prior variance. \n",
    "\n",
    "\n",
    "top_bound = np.array(prior_data[\"fraction_correct_weighted_means\"]) + np.array(prior_data[\"fraction_correct_weighted_stddevs\"])\n",
    "#If any of the top bounds are greater than 1, set them equal to 1\n",
    "top_bound[top_bound > 1] = 1\n",
    "\n",
    "plt.plot(prior_data[\"eci_stddev_values\"], prior_data[\"fraction_correct_weighted_means\"], color='r')\n",
    "plt.fill_between(prior_data[\"eci_stddev_values\"], \n",
    "                 np.array(prior_data[\"fraction_correct_weighted_means\"]) - np.array(prior_data[\"fraction_correct_weighted_stddevs\"]), \n",
    "                 top_bound, \n",
    "                 color='r', \n",
    "                 alpha=0.2)\n",
    "plt.xlabel('Prior stddev (eV)', fontsize=21)\n",
    "plt.ylabel('Fraction Correct Weighted', fontsize=21)\n",
    "plt.xticks(fontsize=21)\n",
    "plt.yticks(fontsize=21)\n",
    "plt.legend([ 'GSA_mean',\"GSA_stddev\"], fontsize=21)\n",
    "\n",
    "#plot vertical lines at every 0.005 eV\n",
    "for i in np.arange(0.005, 0.05, 0.005):\n",
    "    plt.axvline(x=i, color='k', linestyle='--')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plot horizontal lines at every 0.1 \n",
    "for i in np.arange(0.1, 1.1, 0.1):\n",
    "    plt.axhline(y=i, color='k', linestyle='--')\n",
    "\n",
    "\n",
    "\n",
    "#Plot rmse on the same plot with a different y axis\n",
    "ax2 = plt.gca().twinx()\n",
    "ax2.plot(prior_data[\"eci_stddev_values\"], prior_data[\"rmse_means\"], color='k')\n",
    "ax2.fill_between(prior_data[\"eci_stddev_values\"], \n",
    "                 np.array(prior_data[\"rmse_means\"]) - np.array(prior_data[\"rmse_stddevs\"]), \n",
    "                 np.array(prior_data[\"rmse_means\"]) + np.array(prior_data[\"rmse_stddevs\"]), \n",
    "                 color='k', alpha=0.2)\n",
    "#Label the second y axis \n",
    "ax2.set_ylabel('RMSE (eV)', fontsize=21)\n",
    "ax2.tick_params(axis='y', labelcolor='k', labelsize=21)\n",
    "\n",
    "    \n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15,12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the prior stddev where the ground state accuracy mean - 1 stddev drops below 0.9\n",
    "#This is the prior stddev that we will use for the final model\n",
    "prior_stddev_90_percent = prior_data[\"eci_stddev_values\"][np.where(np.array(prior_data[\"fraction_correct_weighted_means\"]) - np.array(prior_data[\"fraction_correct_weighted_stddevs\"]) < 0.9)[0][0]]\n",
    "print(prior_stddev_90_percent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the uncertainty in the posterior model\n",
    "\n",
    "\n",
    "\n",
    "#If you happened to find a model with higher ground state accuracy, \n",
    "#try using the max gsa model as the mean instead of the max model (uncomment below)\n",
    "prior_mean_nonzero = max_gsa_model\n",
    "\n",
    "\n",
    "prior_covariance = prior_stddev_90_percent ** 2 * np.identity(prior_mean_nonzero.shape[0])\n",
    "\n",
    "plt.errorbar(x=list(range(len(prior_mean_nonzero))),\n",
    "    y=prior_mean_nonzero,\n",
    "    yerr=2*np.power(np.diag(prior_covariance),.5),\n",
    "    fmt='.',\n",
    "    color='r',\n",
    "    label='ECI: 2 Standard Deviations')\n",
    "plt.errorbar(x=list(range(len(prior_mean_nonzero))),\n",
    "    y=prior_mean_nonzero,  \n",
    "    yerr=np.power(np.diag(prior_covariance),.5), \n",
    "    fmt='.', \n",
    "    color='k', \n",
    "    label='ECI: 1 Standard Deviation')\n",
    "plt.scatter(list(range(len(prior_mean_nonzero))), prior_mean_nonzero, color='r', label='prior mean')\n",
    "plt.ylim([-.8,0.8])\n",
    "plt.hlines(y=0, xmin=0, xmax=len(prior_mean_nonzero), linestyles='--', color='k')\n",
    "plt.legend(fontsize=21)\n",
    "plt.xticks(fontsize=21)\n",
    "plt.yticks(fontsize=21)\n",
    "plt.xlabel('ECI index', fontsize=21)\n",
    "plt.ylabel('ECI value', fontsize=21)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(20,12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the posterior given the prior selected by sampling \n",
    "max_gsa_model_covariance = prior_stddev_90_percent ** 2 * np.identity(prior_mean_nonzero.shape[0])\n",
    "likelihood_covariance = np.eye(corr_nonzero.shape[0]) * likelihood_stddev**2\n",
    "max_gsa_model_posterior_mean, max_gsa_model_posterior_covariance = dj.analytic_posterior(feature_matrix =corr_nonzero, \n",
    "                                                                                         weight_mean_vec=max_gsa_model,\n",
    "                                                                                        weight_covariance_matrix=max_gsa_model_covariance, \n",
    "                                                                                        label_vec=formation_energy, \n",
    "                                                                                        label_covariance_matrix=likelihood_covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the ground state accuracy of the posterior model \n",
    "posterior_gsa = cl.gsa_fraction_correct_DFT_mu_window_binary(predicted_comp=comp_full, \n",
    "                                                             predicted_energies=corr_full_nonzero @ max_gsa_model_posterior_mean, \n",
    "                                                             predicted_corr=corr_full_nonzero, \n",
    "                                                             true_comp=comp, \n",
    "                                                             true_energies=formation_energy, \n",
    "                                                             true_corr=corr_nonzero)\n",
    "print(posterior_gsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the convex hull of the posterior model\n",
    "max_gsa_model_posterior_predicted_energies = corr_nonzero @ max_gsa_model_posterior_mean\n",
    "fig = general_binary_convex_hull_plotter(composition=comp, true_energies=formation_energy, predicted_energies=max_gsa_model_posterior_predicted_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the posterior mean, posterior covariance and upscaling vector to a json file\n",
    "with open('posterior_data.json', 'w') as f:\n",
    "    json.dump({'posterior_mean':max_gsa_model_posterior_mean.tolist(), \n",
    "               'posterior_covariance':max_gsa_model_posterior_covariance.tolist(), \n",
    "               'upscaling_vector':zeroed_upscaling_boolean_vector.tolist()}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#END OF FIRST OPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way to select the prior stddev is to select the prior stddev that maximizes the posterior ground state accuracy\n",
    "#Again, we can start at the optimal mean, and iteratively broaden the prior distribution. \n",
    "#Every time we broaden the prior, we can calculate the posterior distribution and save the posterior mean model.\n",
    "\n",
    "\n",
    "posterior_means = []\n",
    "for index, eci_stddev in enumerate(eci_stddev_values):\n",
    "    prior_covariance = np.eye(prior_mean_nonzero.shape[0]) * eci_stddev**2\n",
    "    likelihood_covariance = np.eye(corr_nonzero.shape[0]) * likelihood_stddev**2\n",
    "    posterior_mean, posterior_covariance = dj.analytic_posterior(feature_matrix=corr_nonzero,\n",
    "    weight_covariance_matrix=prior_covariance,\n",
    "    weight_mean_vec=prior_mean_nonzero, \n",
    "    label_vec=formation_energy, \n",
    "    label_covariance_matrix=likelihood_covariance)\n",
    "    posterior_means.append(posterior_mean)\n",
    "\n",
    "posterior_means = np.array(posterior_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each mean model, we can calcualte the predicted energies, and the ground state energy of those predictions \n",
    "\n",
    "\n",
    "print(posterior_means.shape)\n",
    "\n",
    "\n",
    "#Calculate the predicted energies for each model \n",
    "posterior_predicted_energies = corr_nonzero @ posterior_means.T\n",
    "posterior_predicted_energies = posterior_predicted_energies.T\n",
    "\n",
    "posterior_predicted_energies_full_enumeration = corr_full_nonzero @ posterior_means.T\n",
    "posterior_predicted_energies_full_enumeration = posterior_predicted_energies_full_enumeration.T\n",
    "\n",
    "print(posterior_predicted_energies.shape)\n",
    "print(posterior_predicted_energies_full_enumeration.shape)\n",
    "\n",
    "\n",
    "posterior_rmse = []\n",
    "posterior_fraction_correct_weighted_full_enumeration = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for index, row in enumerate(posterior_predicted_energies_full_enumeration):\n",
    "#    predicted_hull_full_enumeration = thull.full_hull(compositions=comp_full, energies=row)\n",
    "#    predicted_lower_hull_vertices_full_enumeration, _ = thull.lower_hull(predicted_hull_full_enumeration)\n",
    "\n",
    "    posterior_rmse.append(mean_squared_error(y_true=formation_energy,y_pred=posterior_predicted_energies[index],squared=False))\n",
    "\n",
    "\n",
    "    posterior_fraction_correct_weighted_full_enumeration.append(\n",
    "        cl.gsa_fraction_correct_DFT_mu_window_binary(predicted_comp=comp_full, \n",
    "                                                     predicted_energies=row,\n",
    "                                                     predicted_corr=corr_full,\n",
    "                                                     true_comp=comp,\n",
    "                                                     true_energies=formation_energy,\n",
    "                                                     true_corr=corr)\n",
    "        )\n",
    "posterior_fraction_correct_weighted_full_enumeration = np.array(posterior_fraction_correct_weighted_full_enumeration)\n",
    "posterior_rmse = np.array(posterior_rmse)\n",
    "\n",
    "print(posterior_fraction_correct_weighted_full_enumeration.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot metrics agains the prior standard deviation \n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 12))\n",
    "ax.plot(eci_stddev_values, (1-np.array(posterior_rmse)/max_rmse_tolerance), label='rmse-based accuracy metric')\n",
    "ax.plot(eci_stddev_values, posterior_fraction_correct_weighted_full_enumeration, label='')\n",
    "ax.set_xlabel('eci_stddev', fontsize=21)\n",
    "ax.set_ylabel('metric', fontsize=21)\n",
    "ax.legend(fontsize=21, loc='lower right')\n",
    "plt.xticks(fontsize=21)\n",
    "plt.yticks(fontsize=21)\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the product of relevant metrics vs eci_stddev\n",
    "posterior_product =   posterior_fraction_correct_weighted_full_enumeration *(1-np.array(posterior_rmse)/max_rmse_tolerance)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 12))\n",
    "ax.plot(eci_stddev_values, posterior_product, label='product')\n",
    "ax.set_xlabel('eci_stddev', fontsize=21)\n",
    "ax.set_ylabel('product', fontsize=21)\n",
    "ax.legend(fontsize=21)\n",
    "plt.xticks(fontsize=21)\n",
    "plt.yticks(fontsize=21)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the argmax of the product, find the corresponding eci_stddev\n",
    "optimal_eci_stddev = np.argmax(posterior_product)\n",
    "print(eci_stddev_values[optimal_eci_stddev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the posterior distribution using the optimal eci_stddev\n",
    "prior_covariance = np.eye(prior_mean_nonzero.shape[0]) * eci_stddev_values[optimal_eci_stddev]**2\n",
    "likelihood_covariance = np.eye(corr_nonzero.shape[0]) * likelihood_stddev**2\n",
    "posterior_mean, posterior_covariance = dj.analytic_posterior(feature_matrix=corr_nonzero, \n",
    "weight_covariance_matrix=prior_covariance,\n",
    "weight_mean_vec=prior_mean_nonzero, \n",
    "label_vec=formation_energy, \n",
    "label_covariance_matrix=likelihood_covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the posterior mean model\n",
    "fig = general_binary_convex_hull_plotter(composition=comp, true_energies=formation_energy, predicted_energies=corr_nonzero@posterior_mean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the uncertainty in the posterior model\n",
    "plt.errorbar(x=list(range(len(posterior_mean))),\n",
    "    y=posterior_mean,\n",
    "    yerr=2*np.power(np.diag(posterior_covariance),.5),\n",
    "    fmt='.',\n",
    "    color='r',\n",
    "    label='ECI: 2 Standard Deviations')\n",
    "plt.errorbar(x=list(range(len(posterior_mean))),\n",
    "    y=posterior_mean,  \n",
    "    yerr=np.power(np.diag(posterior_covariance),.5), \n",
    "    fmt='.', \n",
    "    color='k', \n",
    "    label='ECI: 1 Standard Deviation')\n",
    "plt.scatter(list(range(len(posterior_mean))), posterior_mean, color='r', label='posterior mean')\n",
    "plt.ylim([-.8,0.8])\n",
    "plt.hlines(y=0, xmin=0, xmax=len(posterior_mean), linestyles='--', color='k')\n",
    "plt.legend(fontsize=21)\n",
    "plt.xticks(fontsize=21)\n",
    "plt.yticks(fontsize=21)\n",
    "plt.xlabel('ECI index', fontsize=21)\n",
    "plt.ylabel('ECI value', fontsize=21)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(20,12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the posterior mean, posterior covariance matrix and upscaling vector to a json file\n",
    "import json\n",
    "with open('posterior_mean_weighted_LS_with_hullcorr.json', 'w') as f:\n",
    "    write_dict = {}\n",
    "    write_dict['posterior_mean'] = posterior_mean.tolist()\n",
    "    write_dict['posterior_covariance'] = posterior_covariance.tolist()\n",
    "    write_dict['upscaling_vector'] = zeroed_upscaling_boolean_vector.tolist()\n",
    "    json.dump(write_dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thermo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36ecc6b6eb056291f47bae731289e1dabfde8d50092a786ca7d2e4468827f3a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
