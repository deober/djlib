{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import djlib.djlib as dj\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json \n",
    "import thermocore.io.casm as cio\n",
    "import thermocore.geometry.hull as thull\n",
    "import djlib.mc.mc as mc\n",
    "from glob import glob\n",
    "import djlib.clex.clex as cl\n",
    "from djlib.plotting.hull_plotting import plot_stable_chemical_potential_windows_for_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load casm query data\n",
    "with open('ZrN_FCC_1.2.0_8_body_10-5-2022.json') as f:\n",
    "    query = json.load(f)\n",
    "data = cio.regroup_query_by_config_property(query)\n",
    "un_pruned_corr = np.array(data['corr'])\n",
    "corr = np.array(data['corr'])\n",
    "comp = np.array(data['comp'])\n",
    "formation_energy = np.array(data['formation_energy'])\n",
    "name = np.array(data['name'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the posteriror distribution from file. \n",
    "#A separate example in the djlib examples directory shows how to obtain quality fits, and posterior distributions.\n",
    "with open('posterior_mean_weighted_LS_with_hullcorr.json') as f:\n",
    "    posterior = json.load(f)\n",
    "print(posterior.keys())\n",
    "pruned_posterior_mean = np.array(posterior['posterior_mean'])\n",
    "posterior_covariance = np.array(posterior['posterior_covariance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Often, we will work with a large set of basis functions, and remove basis functions that are not needed. \n",
    "#However, CASM will expect an ECI vector that is the same length as the original set of basis functions.\n",
    "#Maintaining an upscaling vector of booleans allows us to record the basis functions that are removed, and\n",
    "#reconstruct the original ECI vector when needed.\n",
    "upscaling_vector = np.array(posterior['upscaling_vector'])\n",
    "print(upscaling_vector.shape)\n",
    "pruned_corr = corr[:,upscaling_vector==1]\n",
    "print(pruned_corr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the convex hull of the posterior distribution, after pruning zeroed ECI and their corresponding basis functions. \n",
    "from djlib.plotting.hull_plotting import general_binary_convex_hull_plotter\n",
    "fig = general_binary_convex_hull_plotter(true_energies=formation_energy, composition=comp, predicted_energies=pruned_corr@pruned_posterior_mean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the convex hull of the un-pruned posterior distribution, to ensure that the pruned posterior is correct.  \n",
    "fig = general_binary_convex_hull_plotter(true_energies=formation_energy, composition=comp, predicted_energies=un_pruned_corr@cl.upscale_eci_vector(pruned_posterior_mean,upscaling_vector))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize ECI uncertainty\n",
    "plt.errorbar(list(range(len(pruned_posterior_mean))), pruned_posterior_mean, yerr=np.sqrt(np.diag(posterior_covariance)), fmt='.', capsize=2, color='r', label='1 standard deviation')\n",
    "plt.errorbar(list(range(len(pruned_posterior_mean))), pruned_posterior_mean, yerr=2*np.sqrt(np.diag(posterior_covariance)), fmt='.', capsize=2, color='k', label='2 standard deviations')\n",
    "plt.xlabel('ECI index', fontsize=21)\n",
    "plt.ylabel('ECI value (eV)', fontsize=21)\n",
    "plt.xticks(fontsize=21)\n",
    "plt.yticks(fontsize=21)\n",
    "plt.legend(fontsize=21)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This propagatin code was written to allow for the propagation of many ECI vectors. \n",
    "#However, the code is also convenient for managing a single propagation. This requires minimal modification, shown below. \n",
    "\n",
    "#Draw a set number of random samples from the posterior distribution\n",
    "number_of_propagations = 1\n",
    "#eci = np.random.multivariate_normal(pruned_posterior_mean, posterior_covariance, number_of_propagations)\n",
    "eci = pruned_posterior_mean.reshape(1,-1)\n",
    "\n",
    "propagation_directory = \"/media/derick/big_storage/research_backup/DeoResearch/experiments/ZrN_FCC_1.2.0_ediffg_-0.02/posterior_phase_diagram\" \n",
    "\n",
    "#Create a list of dictionaries that will be used to create the propagation directories\n",
    "#This dictionary is formatted with keys that are expected by the sgcmc_casm_project_creator function defined below. \n",
    "propagation_info_dicts = [{\"sample_index\":int(i), \n",
    "\"template_project_root_path\":'/media/derick/big_storage/research_backup/DeoResearch/experiments/ZrN_FCC_1.2.0_propagation/template_project', \n",
    "\"eci\":cl.upscale_eci_vector(eci[i,:],upscaling_vector), \n",
    "\"propagation_directory\":propagation_directory} for i in range(eci.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a linearly spaced array of mu values. \n",
    "#For each mu value, a  LTE, heating and cooling run will be performed. \n",
    " \n",
    "#mu_list = np.linspace(-1.5,1.5,31).tolist()\n",
    "#Create a new mu list that runs between 0.31 to 0.49, incrementing by 0.01\n",
    "#mu_list = np.linspace(0.31,0.49,19).tolist()\n",
    "#mu_list = np.linspace(0.01,0.29,29).tolist()\n",
    "mu_list = np.linspace(-0.29,-0.21,9).tolist()\n",
    "\n",
    "print(mu_list)\n",
    "\n",
    "#Create a linearly spaced array of temperature values. \n",
    "#For each temperature value, two  constant temperature runs will be performed, one from low to high chemical potential, and one from high to low chemical potential.\n",
    "#Often, it is only necessary to do one high temperature pair. \n",
    "T_list = [1700]\n",
    "print(T_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a setup function that the gridspace manager will use to set up heating, cooling, LTE and constant temperature runs.\n",
    "#Specific details such as supercell shape, chemical potential ranges, temperature ranges, walltime, etc. are defined here. \n",
    "\n",
    "\n",
    "def sgcmc_setup(casm_root_path: str):\n",
    "    \"\"\"A very specific function: Writes all necessary files for heating and cooling runs for the ground state at 50% composition. This includes:\n",
    "        -High temperature constant t runs from very low to very high chemical potential and very high to very low chemical potential\n",
    "        -Cooling runs from the high temperature constant t runs\n",
    "        -Low Temperature Expansion (LTE) runs\n",
    "        -Heating runs that initialize from LTE runs\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    casm_root_path: str\n",
    "        Path to the casm project root\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a dj.gridspace_manager object to control lte runs. Then format the run directories.\n",
    "    lte_dir = os.path.join(casm_root_path, \"grand_canonical_monte_carlo/MC_LTE\")\n",
    "    lte_param_list_of_dicts = [\n",
    "        {\n",
    "            \"mu_start\": mu,\n",
    "            \"mu_stop\": mu,\n",
    "            \"mu_increment\": 0.0,\n",
    "            \"T_start\": 40.0,\n",
    "            \"T_stop\": 40.0,\n",
    "            \"T_increment\": 0.0,\n",
    "            \"supercell\": [[24, 0, 0], [0, 24, 0], [0, 0, 24]],\n",
    "            \"hours\": 24,\n",
    "        }\n",
    "    for mu in mu_list]\n",
    "    lte_gs = dj.gridspace_manager(\n",
    "        origin_dir=lte_dir,\n",
    "        namer=mc.mc_run_namer,\n",
    "        run_creator=mc.mc_lte_run_creator,\n",
    "        status_updater=mc.mc_status_updater,\n",
    "        run_submitter=mc.mc_run_submitter,\n",
    "        grid_params=lte_param_list_of_dicts,\n",
    "    )\n",
    "    lte_gs.format_run_dirs()\n",
    "\n",
    "    # Create a dj.gridspace_manager object to control high temperature constant t runs. Then format the run directories.\n",
    "    t_const_dir = os.path.join(casm_root_path, \"grand_canonical_monte_carlo/MC_t_const\")\n",
    "    t_const_param_list_of_dicts = [\n",
    "        {\n",
    "            \"mu_start\": 1.5,\n",
    "            \"mu_stop\": -1.5,\n",
    "            \"mu_increment\": -0.01,\n",
    "            \"T_start\": T,\n",
    "            \"T_stop\": T,\n",
    "            \"T_increment\": 0.0,\n",
    "            \"supercell\": [[24, 0, 0], [0, 24, 0], [0, 0, 24]],\n",
    "            \"hours\": 50,\n",
    "        }\n",
    "    for T in T_list]\n",
    "\n",
    "    t_const_param_list_of_dicts += [\n",
    "        {\n",
    "            \"mu_start\": -1.5,\n",
    "            \"mu_stop\": 1.5,\n",
    "            \"mu_increment\": 0.01,\n",
    "            \"T_start\": T,\n",
    "            \"T_stop\": T,\n",
    "            \"T_increment\": 0.0,\n",
    "            \"supercell\": [[24, 0, 0], [0, 24, 0], [0, 0, 24]],\n",
    "            \"hours\": 50,\n",
    "        }\n",
    "    for T in T_list]\n",
    "    t_const_gs = dj.gridspace_manager(\n",
    "        origin_dir=t_const_dir,\n",
    "        namer=mc.mc_run_namer,\n",
    "        run_creator=mc.mc_run_creator,\n",
    "        status_updater=mc.mc_status_updater,\n",
    "        run_submitter=mc.mc_run_submitter,\n",
    "        grid_params=t_const_param_list_of_dicts,\n",
    "    )\n",
    "    t_const_gs.format_run_dirs()\n",
    "\n",
    "    # Create a dj.gridspace_manager object to control cooling runs\n",
    "    cooling_dir = os.path.join(casm_root_path, \"grand_canonical_monte_carlo/MC_cooling\")\n",
    "    cooling_param_list_of_dicts = [\n",
    "        {\n",
    "            \"mu_start\": mu,\n",
    "            \"mu_stop\": mu,\n",
    "            \"mu_increment\": 0.0,\n",
    "            \"T_start\": 1700.0,\n",
    "            \"T_stop\": 40.0,\n",
    "            \"T_increment\": -10.0,\n",
    "            \"supercell\": [[24, 0, 0], [0, 24, 0], [0, 0, 24]],\n",
    "            \"hours\": 36,\n",
    "        }\n",
    "    for mu in mu_list]\n",
    "    cooling_gs = dj.gridspace_manager(\n",
    "        origin_dir=cooling_dir,\n",
    "        namer=mc.mc_run_namer,\n",
    "        run_creator=mc.mc_run_creator,\n",
    "        status_updater=mc.mc_status_updater,\n",
    "        run_submitter=mc.mc_run_submitter,\n",
    "        grid_params=cooling_param_list_of_dicts,\n",
    "    )\n",
    "    cooling_gs.format_run_dirs()\n",
    "\n",
    "    #Make sure the cooling run initializes from the constant temperature run with the closest chemical potential value\n",
    "    #Assuming there are only two constant temperature runs, that they are at the high temperature, and that they cover the same chemical potential\n",
    "    #values (in opposite orders)\n",
    "    #First, look up the constant temperature run with the closest starting chemical potential. \n",
    "    #Then, find the index of the chemical potential with the closest value in the cooling run. \n",
    "    #This index marks the conditions directory that the cooling run should initialize from.\n",
    "    \n",
    "    t_const_runs = np.array(glob(os.path.join(t_const_dir, \"mu_*\")))\n",
    "    t_const_mu = []\n",
    "    t_const_temperatures = []\n",
    "    for t_const_run in t_const_runs:\n",
    "        mu_temporary = mc.read_mc_settings(os.path.join(t_const_run, \"mc_settings.json\"))[0]\n",
    "        temperature_temporary = mc.read_mc_settings(os.path.join(t_const_run, \"mc_settings.json\"))[1][0]\n",
    "        t_const_mu.append(mu_temporary)\n",
    "        t_const_temperatures.append(temperature_temporary)\n",
    "    t_const_mu = np.array(t_const_mu)\n",
    "    t_const_temperatures = np.array(t_const_temperatures)\n",
    "\n",
    "    #find the indices of t_const_temperatures that are equal to 1700 \n",
    "    t_const_1700_indices = np.where(t_const_temperatures == 1700)[0]\n",
    "\n",
    "    #Downsample t_const_mu to only include the mu values at 1700 K\n",
    "    t_const_mu_1700 = t_const_mu[t_const_1700_indices]\n",
    "\n",
    "    #Iterate through all cooling runs\n",
    "    for cooling_run_path in glob(os.path.join(cooling_dir, \"mu_*\")):\n",
    "        #Get the chemical potential from the cooling run\n",
    "        cooling_mu = mc.read_mc_settings(os.path.join(cooling_run_path, \"mc_settings.json\"))[0][0]\n",
    "\n",
    "        #Find the index of the constant temperature run with the closest initial chemical potential\n",
    "        closest_t_const_index = 0\n",
    "\n",
    "        #Find the closest conditions index in the constant temperature run to initialize the cooling run from. \n",
    "        closest_conditions_index = np.argmin(np.abs(t_const_mu_1700[closest_t_const_index] - cooling_mu))\n",
    "\n",
    "        #First, check that the closest conditions file exists. Raise a warning if it does not.\n",
    "        if not os.path.exists(os.path.join(t_const_runs[t_const_1700_indices][closest_t_const_index], \"conditions.%d/final_state.json\" % closest_conditions_index)):\n",
    "            print(\"The closest conditions file does not exist. Check that the constant temperature runs have been run.\")\n",
    "        else:\n",
    "            #Write the closest conditions index to the cooling run's mc_settings.json file\n",
    "            with open(os.path.join(cooling_run_path, \"mc_settings.json\"), \"r\") as f:\n",
    "                cooling_settings = json.load(f)\n",
    "            cooling_settings[\"driver\"][\"motif\"][\"configdof\"] = os.path.join(\n",
    "                t_const_runs[closest_t_const_index], \"conditions.%d/final_state.json\" % closest_conditions_index\n",
    "            )\n",
    "            cooling_settings[\"driver\"][\"motif\"].pop(\"configname\", None)\n",
    "            cooling_settings[\"driver\"][\"motif\"].pop(\"_configname\", None)\n",
    "            cooling_settings[\"driver\"][\"motif\"].pop(\"_configdof\", None)\n",
    "            with open(os.path.join(cooling_run_path, \"mc_settings.json\"), \"w\") as f:\n",
    "                json.dump(cooling_settings, f, indent=4)\n",
    "\n",
    "\n",
    "    # Create a dj.gridspace_manager object to control heating runs\n",
    "    heating_dir = os.path.join(casm_root_path, \"grand_canonical_monte_carlo/MC_heating\")\n",
    "    heating_param_list_of_dicts = [\n",
    "        {\n",
    "            \"mu_start\": mu,\n",
    "            \"mu_stop\": mu,\n",
    "            \"mu_increment\": 0.0,\n",
    "            \"T_start\": 40.0,\n",
    "            \"T_stop\": 1700.0,\n",
    "            \"T_increment\": 10.0,\n",
    "            \"supercell\": [[24, 0, 0], [0, 24, 0], [0, 0, 24]],\n",
    "            \"hours\": 36,\n",
    "        }\n",
    "    for mu in mu_list]\n",
    "    heating_gs = dj.gridspace_manager(\n",
    "        origin_dir=heating_dir,\n",
    "        namer=mc.mc_run_namer,\n",
    "        run_creator=mc.mc_run_creator,\n",
    "        status_updater=mc.mc_status_updater,\n",
    "        run_submitter=mc.mc_run_submitter,\n",
    "        grid_params=heating_param_list_of_dicts,\n",
    "    )\n",
    "    heating_gs.format_run_dirs()\n",
    "\n",
    "def sgcmc_casm_project_creator(\n",
    "    propagation_info_dict, propagation_project_root_path\n",
    "):\n",
    "    \"\"\"Copies a pre-templated casm project, writes a specific eci vector to\n",
    "    project_root/cluster_expansions/clex.formation_energy/calctype.default/ref.default/bset.default/eci.default/eci.json\n",
    "    and creates all standard directories for typical grand canonical monte carlo simulations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    propagation_info_dict : dict\n",
    "        Dictionary containing the following keys:\n",
    "            'template_project_root_path' : str\n",
    "                Path to the casm project root\n",
    "            'markov_chain_index' : int\n",
    "                Index of the eci selection in the posterior markov chain to be used. Also decides the name of the propagaiton directory.\n",
    "            'eci' : np.ndarray\n",
    "                ECI vector to write to the casm project\n",
    "            'propagation_directory' : str\n",
    "                Path to the directory which will contain all the propagation directories.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy the template project to the propagation directory, and name it according to the markov chain index.\n",
    "    template_project_root_path = propagation_info_dict[\"template_project_root_path\"]\n",
    "    os.system(\n",
    "        \"cp -r \"\n",
    "        + template_project_root_path\n",
    "        + \"/.\"\n",
    "        + \" \"\n",
    "        + propagation_project_root_path\n",
    "    )\n",
    "\n",
    "    # Load basis.json\n",
    "    basis_json_path = os.path.join(\n",
    "        template_project_root_path, \"basis_sets/bset.default/basis.json\"\n",
    "    )\n",
    "    with open(basis_json_path, \"r\") as f:\n",
    "        basis_dict = json.load(f)\n",
    "\n",
    "    # Append eci to the basis dictionary\n",
    "    eci = propagation_info_dict[\"eci\"]\n",
    "    eci_dict = cio.append_ECIs_to_basis_data(ecis=eci, basis_data=basis_dict)\n",
    "\n",
    "    # Write the dictionary to eci.json within the new project.\n",
    "    if (\n",
    "        os.path.isfile(\n",
    "            os.path.join(\n",
    "                propagation_project_root_path,\n",
    "                \"cluster_expansions/clex.formation_energy/calctype.default/ref.default/bset.default/eci.default/eci.json\",\n",
    "            )\n",
    "        )\n",
    "        == False\n",
    "    ):\n",
    "        with open(\n",
    "            os.path.join(\n",
    "                propagation_project_root_path,\n",
    "                \"cluster_expansions/clex.formation_energy/calctype.default/ref.default/bset.default/eci.default/eci.json\",\n",
    "            ),\n",
    "            \"w\",\n",
    "        ) as f:\n",
    "            json.dump(eci_dict, f)\n",
    "    else:\n",
    "        print(\"ECI file already exists. Skipping.\")\n",
    "        \n",
    "    # Create a grand canonical monte carlo directory within the new project.\n",
    "    os.system(\n",
    "        \"mkdir \"\n",
    "        + os.path.join(propagation_project_root_path, \"grand_canonical_monte_carlo\")\n",
    "    )\n",
    "\n",
    "    # Write the propagation_info_dict to a json file to run_info.json within the grand_canonical_monte_carlo directory.\n",
    "    tmp_propagation_info_dict = propagation_info_dict.copy()\n",
    "    tmp_propagation_info_dict[\"eci\"] = tmp_propagation_info_dict[\"eci\"].tolist()\n",
    "    with open(\n",
    "        os.path.join(\n",
    "            propagation_project_root_path, \"grand_canonical_monte_carlo/run_info.json\",\n",
    "        ),\n",
    "        \"w\",\n",
    "    ) as f:\n",
    "        json.dump(tmp_propagation_info_dict, f)\n",
    "    del tmp_propagation_info_dict\n",
    "\n",
    "    # If it doesn't exist, create a status.json file in the grand canonical monte carlo directory to keep track of all monte carlo run statuses.\n",
    "    if not os.path.exists(\n",
    "        os.path.join(\n",
    "            propagation_project_root_path, \"grand_canonical_monte_carlo/status.json\"\n",
    "        )\n",
    "    ):\n",
    "        with open(\n",
    "            os.path.join(\n",
    "                propagation_project_root_path, \"grand_canonical_monte_carlo/status.json\"\n",
    "            ),\n",
    "            \"w\",\n",
    "        ) as f:\n",
    "            json.dump({}, f)\n",
    "\n",
    "    # Create an MC_cooling, MC_heating, MC_LTE, and MC_t_const directories within the new grand canonical monte carlo directory.\n",
    "    os.system(\n",
    "        \"mkdir \"\n",
    "        + os.path.join(\n",
    "            propagation_project_root_path, \"grand_canonical_monte_carlo/MC_cooling\"\n",
    "        )\n",
    "    )\n",
    "    os.system(\n",
    "        \"mkdir \"\n",
    "        + os.path.join(\n",
    "            propagation_project_root_path, \"grand_canonical_monte_carlo/MC_heating\"\n",
    "        )\n",
    "    )\n",
    "    os.system(\n",
    "        \"mkdir \"\n",
    "        + os.path.join(\n",
    "            propagation_project_root_path, \"grand_canonical_monte_carlo/MC_LTE\"\n",
    "        )\n",
    "    )\n",
    "    os.system(\n",
    "        \"mkdir \"\n",
    "        + os.path.join(\n",
    "            propagation_project_root_path, \"grand_canonical_monte_carlo/MC_t_const\"\n",
    "        )\n",
    "    )\n",
    "    # Set up MC runs for my specific project\n",
    "    sgcmc_setup(propagation_project_root_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import djlib.propagation.propagate_gcmc as pg\n",
    "import djlib.djlib as dj\n",
    "\n",
    "# Create a gridspace manager to manage the propagation of the grand canonical monte carlo simulations.\n",
    "#A gridspace manager is an abstracted calss, made to handle many repetitive calculations across gridspaces. \n",
    "#Provided with the necessary user-made functions, it will handle directory creation, status updating, job submission, and result parsing. \n",
    "#The namer, status updater, submitter and parser are already written and generally applicable. However, the run creator and grid parameters vary from project to project.\n",
    "#Please see the examples above to get a better idea of how these work. \n",
    "#For more information, please see the documentation for the gridspace manager class.\n",
    "\n",
    "propagation_grid_space_manager = dj.gridspace_manager(origin_dir=propagation_directory,\n",
    "    namer=pg.propagation_project_namer,\n",
    "    status_updater=pg.propagation_casm_project_status_updater, \n",
    "    run_creator=sgcmc_casm_project_creator, \n",
    "    grid_params=propagation_info_dicts,\n",
    "    run_submitter=pg.propagation_casm_project_submitter,\n",
    "    run_parser=pg.propagation_project_parser,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and format all directories and files necesary for calculations.\n",
    "#NOTE: If this function runs while jobs are active in slurm, the job will cancel and must be restarted.\n",
    "propagation_grid_space_manager.format_run_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the status of all runs. \n",
    "#NOTE: This function does not distinguish between jobs that still running but un-finished, and jobs that are not running and incomplete. Both are considered incomplete.\n",
    "#NOTE: If the status of a heating run or LTE run is still \"not_submitted\" and you are sure that you submitted it, there was likely a segmentation fault in CASM. \n",
    "#This is a supercell shape issue. For a heating or LTE run at a given chemical potential,CASM will try to find a supercell that has the lowest formation energy. \n",
    "#If multiples of this supercell cannot fit inside the supercell specified in the mc_settings.json file, CASM will crash. \n",
    "propagation_grid_space_manager.update_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit all jobs that are not submitted.\n",
    "#Additionally, since cooling runs must initialize from high temperature constant temperature runs, \n",
    "#the cooling runs will not submit until the constant temperature runs are complete.\n",
    "#You must first submit the constant temperature runs. Once they are complete, run the format_run_dirs() and update_status() methods again, and then run this method.\n",
    "propagation_grid_space_manager.run_valid_calculations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect project data and examine relevant \"diagnostic\" plots for the propagation project.\n",
    "\n",
    "\n",
    "from djlib.plotting.mc_plotting import sgcmc_full_project_diagnostic_plots\n",
    "sample_index = 0  \n",
    "sample_name = 'sample_index_' + str(sample_index)\n",
    "project_path = os.path.join(propagation_directory, sample_name)\n",
    "print(project_path)\n",
    "\n",
    "#Collect data. By default, the parser will wait until all jobs are complete before parsing.\n",
    "#You should NOT make a phase diagram with incomplete data. \n",
    "#If you only want to visualize cooling plots for a project, you can set incomplete_override=True to parse incomplete data.\n",
    "data = pg.propagation_project_parser(project_path, incomplete_override=True)\n",
    "print(data.keys())\n",
    "integrated_data = mc.full_project_integration(data)\n",
    "\n",
    "#Plot diagnostic plots.\n",
    "fig = sgcmc_full_project_diagnostic_plots(integrated_data,show_legends=False)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the integrated data to a pickle file. This contains all thermodynamic data from all runs in the project. \n",
    "import pickle\n",
    "with open(os.path.join(project_path, 'ZrN_FCC_integrated_data.pkl'), 'wb') as f:\n",
    "    pickle.dump(integrated_data, f)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casm_1.2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7f8c89180b1b01809ebef4d09a4b295f9e97d7edb55f9d07ba3341339b679cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
